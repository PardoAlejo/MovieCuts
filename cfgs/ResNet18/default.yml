# ---------------------------------------------------------------------------- #
# data params
# ---------------------------------------------------------------------------- #
data:
  videos_path: './data/framed_clips'
  cache_path: './.cache'
  shots_file_train: 'data/annotated_clips_train.csv'
  shots_file_val: 'data/annotated_clips_val.csv'
  cut_type_file_name_train: 'data/cut-type-train.json'
  cut_type_file_name_val: 'data/cut-type-val.json'
  cut_type_file_name_test: 'data/cut-type-test.json'
  scale_h: 128 # Scale H to read
  scale_w: 180 # Scale W to read
  crop_size: 112 # crop size to input the network
  snippet_size: 16 # number of frames per clip
  num_classes: 10
  window_sampling: fixed # choose between ['gaussian', 'uniform', 'fixed']
  distribution: natural # choose between ['natural', 'uniform', 'sqrt']
  data_percent: 1 # *100 percetange of data to train with
  negative_portion: 0

# ---------------------------------------------------------------------------- #
# Networks params
# ---------------------------------------------------------------------------- #
model:
    visual_stream: True
    audio_stream: True
    num_visual_layers: 18
    num_audio_layers: 18
    abeta: 0.6
    avbeta: 0.18
    vbeta: 0.22
    
# ---------------------------------------------------------------------------- #
# Inference options
# ---------------------------------------------------------------------------- #

inference:
  print_freq: 1
  validation: True 
  test: False
  checkpoint: ''
  sanity_check_steps: 0
  multi_modal_inference: 0
  save_path: 
# ---------------------------------------------------------------------------- #
# Training options
# ---------------------------------------------------------------------------- #

training:
  num_workers: 16
  max_epochs: 10
  initialization: supervised #['scratch', 'supervised']
  print_freq: 1
  val_freq: 1
  save_freq: 1
  save_top_k: 2

  from_scratch: False
  video_model_path: 'checkpoints/r2plus1d_18-91a641e6.pth'
  audio_model_path: 'checkpoints/vggsound_avgpool.pth.tar'
  
  sanity_check_steps: 0

# ---------------------------------------------------------------------------- #
# Optimizer/ Scheduler options
# ---------------------------------------------------------------------------- #
batch_size: 96 # batch size Per GPU
num_workers: 16

optimizer:
  name: 'sgd'
  weight_decay: 0.0001
  momentum: 0.9

lr_scheduler:
  lr_decay: 0.9
  initial_lr: 0.01  # for 1 GPU and batch size 8. have to manually change when increase the number of GPUs or Batch size.
  warmup_epochs: 1
  lr_gamma: 0.01
  lr_milestones:
  - 4
  - 8

# ---------------------------------------------------------------------------- #
# logging
# ---------------------------------------------------------------------------- #
log_dir: './log'
base_exp_dir: './experiments'
wandb:
  project: moviecuts  # name of the wandb project
  entity: pardoalejo  # *will automatically merge with the parent yaml*
